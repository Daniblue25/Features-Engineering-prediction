{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0UJumCEgbIV"
      },
      "source": [
        "## Importation des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOOX1A9agbIZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRrBLlrwgbIb"
      },
      "source": [
        "## Charger les données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhf0bQ-OgbIb",
        "outputId": "9348daa7-39dd-41ff-a645-08204faea5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 7s 0us/step\n",
            "17473536/17464789 [==============================] - 7s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_V5OWbzgbId"
      },
      "source": [
        "## Préparation de la donnée pour l'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn5CcKH4gbId"
      },
      "outputs": [],
      "source": [
        "max_words = 1000  # Nombre maximum de mots dans une revue\n",
        "training_data = sequence.pad_sequences(training_data, maxlen=max_words)\n",
        "testing_data = sequence.pad_sequences(testing_data, maxlen=max_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEryzSipgbIe"
      },
      "source": [
        "## Création du réseau de neurones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMc-KCldgbIe"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32, input_length=max_words))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAjVcvxWgbIe",
        "outputId": "fa0b376b-2c72-4bc4-e4f7-214a9903c848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1000, 32)          320000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                24832     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 344,897\n",
            "Trainable params: 344,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7bdV0LMgbIf"
      },
      "source": [
        "## Entraînement et évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6rXEUCogbIf",
        "outputId": "a7103334-435a-4510-cf0c-64e68495b063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "313/313 [==============================] - 674s 2s/step - loss: 0.4677 - accuracy: 0.7627 - val_loss: 0.3586 - val_accuracy: 0.8536\n",
            "Epoch 2/3\n",
            "313/313 [==============================] - 505s 2s/step - loss: 0.2523 - accuracy: 0.9003 - val_loss: 0.3023 - val_accuracy: 0.8712\n",
            "Epoch 3/3\n",
            "313/313 [==============================] - 499s 2s/step - loss: 0.1831 - accuracy: 0.9337 - val_loss: 0.3208 - val_accuracy: 0.8710\n",
            "782/782 [==============================] - 212s 271ms/step - loss: 0.3389 - accuracy: 0.8650\n",
            "Accuracy: 86.50\n"
          ]
        }
      ],
      "source": [
        "model.fit(training_data, training_targets, validation_split=0.2, epochs=3, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DimFnRgbIg"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD45xyJVgbIg",
        "outputId": "c4788ba3-4645-424c-cdb3-c257955f8c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "1654784/1641221 [==============================] - 1s 1us/step\n"
          ]
        }
      ],
      "source": [
        "sample_review = \"This movie is fantastic! I loved every minute of it.\"\n",
        "words = sample_review.lower().split()\n",
        "words_indices = [[imdb.get_word_index()[word] if word in imdb.get_word_index() else 0] for word in words]\n",
        "padded_indices = sequence.pad_sequences(words_indices, maxlen=max_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FH4Qh_agbIg",
        "outputId": "57f85669-066b-4412-8b9f-3bb29b420cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sentiment of the review is: Positive\n"
          ]
        }
      ],
      "source": [
        "prediction = model.predict(padded_indices)[0][0]\n",
        "sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "print(\"The sentiment of the review is:\", sentiment)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dani_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}